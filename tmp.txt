python3 seq2seq_attention.py --mode test --model_dir ../../../pytorch_data/09_13_2218 --encoder=encoder_2.pth --decoder=decoder_2.pth


decoder_output.data

output = F.log_softmax(self.out(output[0]), dim=1)
なのでsoftmaxで0~1にしたあと、logで -∞~0

<class 'torch.Tensor'>
torch.Size([1, 30005])
tensor([-11.0001,  -2.5442,  -2.7432,  ..., -10.7662, -10.3689,
        -11.1510])




	東大ロボが使用していた手法はKenLM（単なる統計情報？）
	https://github.com/kpu/kenlm
	https://arxiv.org/pdf/1601.01272.pdf

	つまりは言語モデルでできるとのことなので他の言語モデルもいくつか調べた

	RNNLM ToolKit（word2vecの人が作ったものらしい）
	http://www.fit.vutbr.cz/~imikolov/rnnlm/
	https://qiita.com/s_chag11/items/30cc939d159ad185f8e1

	attentionつきのRNNLM（↑とは別の人）
	https://github.com/ketranm/RMN

	そしてこの論文に他のテストデータセット載ってた
	https://stackoverflow.com/questions/44373470/get-the-microsoft-research-sentence-completion-challenge
	https://dl.acm.org/citation.cfm?id=2390940.2390944


	今後はこのテストデータも使う？
	ベースラインとして自分の学習データとテストデータでKenLM，RNNLM，attentionつきRNNLMをやってみる？


https://yidatao.github.io/2017-05-31/kenlm-ngram/
この方法でkenLM、選択肢Bの方法ならできそうでは？



--- インストール手順とか ---

---- KenLM ----

git clone https://github.com/kpu/kenlm
sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev
cd kenlm
mkdir -p build
cd build
cmake ..
make -j 4
ここまででインストール完了

bin/lmplz -o 5 -S 80% <hoge.txt >hoge.arpa
学習データhoge.txt から モデルhoge.arpaを作成

cd ..
sudo python3 setup.py install

これでpythonからモジュールとして呼び出せる
↓のように
import kenlm
model = kenlm.Model('lm/test.arpa')
print(model.score('this is a sentence .', bos = True, eos = True))


以下、要検討
このeosが文の終わりを示すやつだからeos=Falseにすれば前方からの予測できるのでは？

time bin/lmplz -o 5 -S 80% <../../pytorch_data/text8.txt >text8.arpa

real	0m58.034s
user	0m46.156s
sys	0m11.232s

---- RNNLM ToolKit ----







---- attentionつきのRNNLM ----






a
