python3 seq2seq_attention.py --mode test --model_dir ../../../pytorch_data/09_13_2218 --encoder=encoder_2.pth --decoder=decoder_2.pth


decoder_output.data

output = F.log_softmax(self.out(output[0]), dim=1)
なのでsoftmaxで0~1にしたあと、logで -∞~0

<class 'torch.Tensor'>
torch.Size([1, 30005])
tensor([-11.0001,  -2.5442,  -2.7432,  ..., -10.7662, -10.3689,
        -11.1510])




	東大ロボが使用していた手法はKenLM（単なる統計情報？）
	https://github.com/kpu/kenlm
	https://arxiv.org/pdf/1601.01272.pdf

	つまりは言語モデルでできるとのことなので他の言語モデルもいくつか調べた

	RNNLM ToolKit（word2vecの人が作ったものらしい）
	http://www.fit.vutbr.cz/~imikolov/rnnlm/
	https://qiita.com/s_chag11/items/30cc939d159ad185f8e1

	attentionつきのRNNLM（↑とは別の人）
	https://github.com/ketranm/RMN

	そしてこの論文に他のテストデータセット載ってた
	https://stackoverflow.com/questions/44373470/get-the-microsoft-research-sentence-completion-challenge
	https://dl.acm.org/citation.cfm?id=2390940.2390944


	今後はこのテストデータも使う？
	ベースラインとして自分の学習データとテストデータでKenLM，RNNLM，attentionつきRNNLMをやってみる？


https://yidatao.github.io/2017-05-31/kenlm-ngram/
この方法でkenLM、選択肢Bの方法ならできそうでは？



--- インストール手順とか ---

---- KenLM ----

git clone https://github.com/kpu/kenlm
sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev
cd kenlm
mkdir -p build
cd build
cmake ..
make -j 4
ここまででインストール完了

bin/lmplz -o 5 -S 80% <hoge.txt >hoge.arpa
学習データhoge.txt から モデルhoge.arpaを作成

cd ..
sudo python3 setup.py install

これでpythonからモジュールとして呼び出せる
↓のように
import kenlm
model = kenlm.Model('lm/test.arpa')
print(model.score('this is a sentence .', bos = True, eos = True))


以下、要検討
このeosが文の終わりを示すやつだからeos=Falseにすれば前方からの予測できるのでは？

time bin/lmplz -o 5 -S 80% <../../pytorch_data/text8.txt >text8.arpa

real	0m58.034s
user	0m46.156s
sys	0m11.232s

---- RNNLM ToolKit ----







---- attentionつきのRNNLM ----

まずtorchのインストールから
https://ensekitt.hatenablog.com/entry/2017/01/24/003609
https://qiita.com/perrying/items/c53864d1bd2155a0635d

git clone https://github.com/torch/distro.git ~/torch --recursive
cd ~/torch; bash install-deps;

↑わりと時間かかる（15分くらい？正確には数えてない）

./install.sh

↑こっちもわりと時間かかる（10分くらい？正確には数えてない）

source ~/.bashrc

ここからattentionつきのRNNLMのやつ
git clone https://github.com/ketranm/RMN
cd RMN



th RM.lua -max_seq_length 80 -min_seq_length 10 -max_epochs 20 \
-data_dir ../pytorch_data/text8.txt -print_every 200 -num_layers 1 -mem_size 15  \
-learning_rate 1 -emb_size 128 -rnn_size 128  -nhop 1 \
-checkpoint_dir checkpoint

これでは不可
いろいろエラー言われる

use gating combination
package cunn not found!
package cutorch not found!
If cutorch and cunn are installed, your CUDA toolkit may be improperly configured.
Check your CUDA toolkit installation, rebuild cutorch and cunn, and try again.
Falling back on CPU mode
vocab.t7 and data.t7 do not exist. Running preprocessing...
one-time setup
loading text file...
/home/tamaki/torch/install/bin/luajit: ./text/TextProcessor.lua:111: bad argument #1 to 'lines' (../pytorch_data/text8.txt/train.txt: Not a directory)
stack traceback:
	[C]: in function 'lines'
	./text/TextProcessor.lua:111: in function 'text_to_tensor'
	./text/TextProcessor.lua:44: in function 'create'
	RM.lua:89: in main chunk
	[C]: in function 'dofile'
	...maki/torch/install/lib/luarocks/rocks/trepl/scm-1/bin/th:150: in main chunk
	[C]: at 0x00405d50
