python3 seq2seq_batch.py --mode test --model_dir ../../../pytorch_data/10_09_1136mini --encoder=encoder_100.pth --decoder=decoder_100.pth

time python3.6 seq2seq_attention_batch.py --mode test --model_dir ../../../pytorch_data/10_10_2344all/ --encoder encoder_74.pth --decoder decoder_74.pth

Calc scores ...
  acc(all):  0.00  %
acc(cloze):  0.00  %
 acc(part):  0.00  %
 BLEU:  1.08
  all:  0
cloze:  0
 part:  0
 line:  324
 miss:  324

  acc(all):  0.62  %
acc(cloze):  0.62  %
 acc(part):  3.70  %
 BLEU:  88.32
  all:  2
cloze:  2
 part:  12
 line:  324
 miss:  18

  acc(all):  9.57  %
acc(cloze):  9.57  %
 acc(part):  16.98  %
 BLEU:  85.37
  all:  31
cloze:  31
 part:  55
 line:  324
 miss:  0



	そしてこの論文に他のテストデータセット載ってた
	https://stackoverflow.com/questions/44373470/get-the-microsoft-research-sentence-completion-challenge
	https://dl.acm.org/citation.cfm?id=2390940.2390944


	今後はこのテストデータも使う？
	ベースラインとして自分の学習データとテストデータでKenLM，RNNLM，attentionつきRNNLMをやってみる？


  https://yidatao.github.io/2017-05-31/kenlm-ngram/
  この方法でkenLM、選択肢Bの方法ならできそうでは？


  Traceback (most recent call last):
    File "baseline_RNNLM_ngram.py", line 384, in <module>
      train_loss = train(ntokens, train_data)
    File "baseline_RNNLM_ngram.py", line 269, in train
      loss = criterion(output.view(-1, ntokens), targets)
    File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py", line 491, in __call__
      result = self.forward(*input, **kwargs)
    File "/usr/local/lib/python3.5/dist-packages/torch/nn/modules/loss.py", line 759, in forward
      self.ignore_index, self.reduce)
    File "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py", line 1442, in cross_entropy
      return nll_loss(log_softmax(input, 1), target, weight, size_average, ignore_index, reduce)
    File "/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py", line 1330, in nll_loss
      .format(input.size(0), target.size(0)))
  ValueError: Expected input batch_size (20) to match target batch_size (1).
